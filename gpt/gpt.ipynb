{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "- GPT from scratch! What is the neural network under the hood that sequences the words? \n",
    "- GPT stands for generatively pretrained transformer. The transformer is the neural net.\n",
    "- chatGPT is trained on a good chunk of internet and goes thru pre and post training. \n",
    "- We will do a character level languague model. We will use a smaller dataset, tiny shakespeare, which is a concatenation of all of Shakespeare's work in one file (~1MB).\n",
    "\n",
    "## Attention is all you need\n",
    "- Check out this 2017 landmark AI paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762). \n",
    "- This proposed the Transformer architecture which revolutioned natural language processing (NLP) by relying on self-attention mechanisms.\n",
    "- The Transformer ues scaled dot-product attention to weigh the importance of different words in a sequence. This enables parallelization and handling of long-range dependencies.\n",
    "- This self-attention mechanism is the core innovation in the Transformer model .\n",
    "- Multiple attention heads allow the model to focus on different parts of a sequence simultaneously. This captures diverse linguistic patterns.\n",
    "- Positional encodings are added to input embeddings in order to preserve word order information.\n",
    "- Transformer block comtains feed-forward layers and layer normalization to stabilize training.\n",
    "\n",
    "## Self-attention steps\n",
    "Given an input sequence represented as word embeddings $X$.\n",
    "1. Compute Query $Q$, Key $K$, and Value $V$ metrics. \n",
    "    - Query represents what this word is looking for in other words. $Q = X W_{Q}$\n",
    "    - Key represent what this word contains that other words might find useful. $K = X W_{K}$\n",
    "    - Value represents the actual information in the word. $V = X W_{V}$\n",
    "2. Computer attention scores (scaled dot-product attention)\n",
    "    - $Scores = Q K^T$\n",
    "    - Each row of scores represents how much attention each word should pay to other words.\n",
    "    - This is scaled to prevent extremely high gradients.\n",
    "    - $Scaled Scores = \\frac{Q K^T}{ \\sqrt{d _{k} } }$ where $d_{k}$ is the dimension of the key vectors.\n",
    "3. Apply Softmax to get attention weights.\n",
    "    - This is done to convert the scores into probabilities. The sum of attention weights for each word will be 1.\n",
    "    - $Attention Weights = softmax( \\frac{Q K^T}{ \\sqrt{d _{k} } } )$\n",
    "4. Computer the Weighted Sum of values\n",
    "    - This is each word's final representation.\n",
    "    - $Output = Attention Weights * V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in chracters: 1115394 \n",
      "-----\n",
      "\n",
      "first 100 chracters:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "-----\n",
      "\n",
      "chars:\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size=65\n",
      "\n",
      "-----\n",
      "\n",
      "encode(\"hii there cutie!\")=[46, 47, 47, 1, 58, 46, 43, 56, 43, 1, 41, 59, 58, 47, 43, 2]\n",
      "decode(encode(\"hii there cutie!\"))='hii there cutie!'\n",
      "\n",
      "-----\n",
      "\n",
      "data.shape=torch.Size([1115394]), data.dtype=torch.int64\n",
      "data[:100]=tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in chracters:\", len(text), \"\\n-----\\n\")\n",
    "print(\"first 100 chracters:\\n\", text[:100], \"\\n-----\\n\")\n",
    "\n",
    "# get unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)  # this is the numebr of possible options for next characer in sequence\n",
    "print(\"chars:\")\n",
    "print(''.join(chars))\n",
    "print(f'{vocab_size=}')\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "# create mapping from chracters to integers and back\n",
    "# aka this is a simple \"tokenizer\"\n",
    "# other tokenizers include:\n",
    "# - Google [sentencepiece](https://github.com/google/sentencepiece). This is a subword unit level, aka not chracters, not words.\n",
    "# - OpenAI's [tiktoken](https://github.com/openai/tiktoken) you can import this directly in python code\n",
    "# subword encodings are popular in practice\n",
    "# below is a character level encoder, small code book (65), long sequences\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: takes a list of integers, output a string\n",
    "print(f'{encode(\"hii there cutie!\")=}')\n",
    "print(f'{decode(encode(\"hii there cutie!\"))=}')\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "# encode the entire txt dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'{data.shape=}, {data.dtype=}')\n",
    "print(f'{data[:100]=}')  # direct translation of first 100 characters\n",
    "# this is a massive sequnce of integers representing the data characters\n",
    "# aka the entire dataset of text is now represented as a series of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ADN VAL DATA\n",
    "# split up the data into train and validation sets\n",
    "# this will allow us to understand when the model is overfitting\n",
    "n = int(0.9*len(data))  # first 90% will be train data, rest will be validation data\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 9 characters in sequence\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# BLOCK SIZE\n",
    "# we will never feed whole dataset into transformer to train it\n",
    "# we train the transformer on chunks of the dataset\n",
    "block_size = 8\n",
    "print(\"first 9 characters in sequence\")\n",
    "print(train_data[:block_size+1])\n",
    "# this has multiple examples packed into it and the transformer will train at every position\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]  # targets for each posiiton\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=4, block_size=8\n",
      "\n",
      "inputs to transformer:\n",
      "each row is a chunk of the data training set\n",
      "xb.shape=torch.Size([4, 8])\n",
      "xb=tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets\n",
      "yb.shape=torch.Size([4, 8])\n",
      "yb=tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "When input is [24] the target: 43\n",
      "When input is [24, 43] the target: 58\n",
      "When input is [24, 43, 58] the target: 5\n",
      "When input is [24, 43, 58, 5] the target: 57\n",
      "When input is [24, 43, 58, 5, 57] the target: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "When input is [44] the target: 53\n",
      "When input is [44, 53] the target: 56\n",
      "When input is [44, 53, 56] the target: 1\n",
      "When input is [44, 53, 56, 1] the target: 58\n",
      "When input is [44, 53, 56, 1, 58] the target: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52] the target: 58\n",
      "When input is [52, 58] the target: 1\n",
      "When input is [52, 58, 1] the target: 58\n",
      "When input is [52, 58, 1, 58] the target: 46\n",
      "When input is [52, 58, 1, 58, 46] the target: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "When input is [25] the target: 17\n",
      "When input is [25, 17] the target: 27\n",
      "When input is [25, 17, 27] the target: 10\n",
      "When input is [25, 17, 27, 10] the target: 0\n",
      "When input is [25, 17, 27, 10, 0] the target: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# BLOCK SIZE AND BATCH SIZE\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # the maximum context length for predictions\n",
    "print(f'{batch_size=}, {block_size=}')\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # 4 numbers generated betw 0 and len(data) - block_size, aka offsets in data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('\\ninputs to transformer:')\n",
    "print(\"each row is a chunk of the data training set\")\n",
    "print(f'{xb.shape=}')\n",
    "print(f'{xb=}')\n",
    "print('\\ntargets')\n",
    "print(f'{yb.shape=}')\n",
    "print(f'{yb=}')\n",
    "\n",
    "print('-----')\n",
    "\n",
    "for b in range(batch_size):  # batch_dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'When input is {context.tolist()} the target: {target}')\n",
    "\n",
    "# the 4x8 array contains 32 independent examples in a batch of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 65])\n",
      "loss=tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "idx of 0 corresponds to character '\\n'\n",
      "generate: \n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        # logits are scores for next character in the sequence\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) aka (Batch, time, channel) like (batch=4, time=8 aka block size, channels=65 aka vocab size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # the loss is defined by negative log likelihood, also called cross entrypy \n",
    "            # loss defines the quality of the logits wrt the targets, aka how well are we predicitng the next character\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html \n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "            # for multi dimensional input to cross entropy, channels is needed as the 2nd dimension, aka wants a (B, C, T)\n",
    "            # so reshape the logits\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # stretches out the array to 2D\n",
    "            # targets is of shape (B, T) and we want one dimension B*T, could also just use -1 in view for targets\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # this function is written to be general for now\n",
    "    # we fed entire sequence into model, but then looked at the last piece to predict the next, in this case\n",
    "    # later - we'll have the history be used to predict the next character\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictons\n",
    "            logits, loss = self(idx)  # self(idx) calls forward() function\n",
    "            # focus only on the last time step, aka get last position in the time dimension, because that is all that we are using to predict the next character\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution, aka get a simple prediction for each batch example\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1), dim=1 is the time dimension\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f'{logits.shape=}')  # torch.Size([4, 8, 65]) indicates logitis for every 4,8 position\n",
    "print(f'{loss=}')  # comes out to 4.8786\n",
    "idx = torch.zeros((1,1), dtype=torch.long)  # create 1by1 tensor, B 1, T 1, torch.long is an integer, zero also corresponds to a new line character\n",
    "print(f'idx of {0} corresponds to character {repr(itos[0])}')\n",
    "# generate works on level of batches, so index into 0th row\n",
    "print(\"generate:\", decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  # spits out garbage since model is random now\n",
    "\n",
    "# we have a vocab size of 65, so expected loss is -ln(1/65) wich is about 4.17\n",
    "# so this tells us initial predicitons are diffuse and we are guessing wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# create a PyTrch optimizer object - in makemore, we only used the simple stochastic gradient descent instead\n",
    "# adam is a popular optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# the optimizer object will optimize the parameters using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # zero out all the gradients from the previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # getting the gradients for all the parameters\n",
    "    loss.backward()\n",
    "    # use gradients to optimize parameters\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecor\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))  # spits out garbage since model is random now\n",
    "# this is a simple bigram model since tokens are not talking to each other. we are only looking at the very last character to predict next.\n",
    "# the bigram model, training, and sample generation is summarized in bypgram.py\n",
    "# now, let's get these tokens to start talking to each other --> Transformer time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE MATHEMATICAL TRICK IN SELF-ATTENTION\n",
    "# consider the following toy example\n",
    "# we want to couple tokens in a specific way, 5th token should talk to 4th token and bach, no info from future\n",
    "# simplest way to do to this is use an average of all the precedding tokens\n",
    "# this is extremely weak/lossy though, aka we lose spatial information about tokens\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time (tokens), channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0]=tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "xbow[0]=tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "notice how x[0] and xbow[0] are the same in the first row, since taking average of one token\n",
      "each row of xbow is an average of all precedding elements\n"
     ]
    }
   ],
   "source": [
    "# VERSION 1: double for loop\n",
    "# We want x[b,t] = mean_{i<=t} x[b, i]\n",
    "# bow means bag of words, aka theres a word stored in each of 8 locations and we'll average them\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t,C), row at b, columns everything up to and including the t token\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # avergaing out on time, get bag C dimensional vector to store in xbow\n",
    "\n",
    "print(f'{x[0]=}')\n",
    "print(f'{xbow[0]=}')\n",
    "print(\"notice how x[0] and xbow[0] are the same in the first row, since taking average of one token\")\n",
    "print(\"each row of xbow is an average of all precedding elements\")\n",
    "# this is ineffcient though, we want to use matrix multiplication instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lower triangular portion of tensor given\n",
    "# zeros out upper triangular portion\n",
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)  # in 1th dimension, set keepdim to True, rows will now sum to 1\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # (3,3) @ (3,2) -> (3, 2)\n",
    "print(f'{a=}\\n---\\n{b=}\\n---\\n{c=}')\n",
    "# ah, genius\n",
    "# so now getting averages in incremental fashion in c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "wei shows how much of every row we want to avergae up; note rows sum to one\n",
      "wei.shape=torch.Size([8, 8]), x.shape=torch.Size([4, 8, 2])\n",
      "wei=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "xbow2=tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n",
      "torch.allclose(xbow, xbow2)=True\n"
     ]
    }
   ],
   "source": [
    "# VERSION 2: batch matrix multiplication\n",
    "# use batch multilpy to make a weighted aggregation and the weights are specified in the TxT array\n",
    "# we do weighted sums tha take on the triangular form, so that tokens only take info from preceeding tokens\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time (tokens), channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(f'{x.shape}')\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))  # wei is short for weights\n",
    "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "\n",
    "print(\"wei shows how much of every row we want to avergae up; note rows sum to one\")\n",
    "print(f'{wei.shape=}, {x.shape=}')\n",
    "print(f'{wei=}')\n",
    "print(f'{xbow2=}')\n",
    "print(f'{torch.allclose(xbow, xbow2)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei as all zeros:\n",
      "wei=tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "mask wei with inf where tril is 0:\n",
      "wei=tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "softmax exponentiates every element and divides by the sum, aka normalize\n",
      "softmax wei along 1st dimension:\n",
      "wei=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "xbow3 = wei @ x\n",
      "torch.allclose(xbow, xbow3)=True\n"
     ]
    }
   ],
   "source": [
    "# VERSION 3: USE SOFTMAX\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))  # think of wei as affinities, set as zero by us here, but this will be data dependent, some tokens will find other tokens more or less interesting to different amounts, aka affinities\n",
    "print(f'wei as all zeros:\\n{wei=}\\n')\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # this line is saying tokens from the past cannot communicate\n",
    "print(f'mask wei with inf where tril is 0:\\n{wei=}\\n')\n",
    "\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(\"softmax exponentiates every element and divides by the sum, aka normalize\")\n",
    "print(f'softmax wei along 1st dimension:\\n{wei=}\\n')\n",
    "\n",
    "xbow3 = wei @ x  # this is the aggregation of the values through matrix multiplication\n",
    "print(\"xbow3 = wei @ x\")\n",
    "print(f'{torch.allclose(xbow, xbow3)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR: the preview to self-attention\n",
    "- you can do weighted aggregations of past elements by using matrix multiplcation of a lower traingular fashion\n",
    "- elements in lower traingular part tells you how much each element fuses\n",
    "- we actually don't want wei to be init to all zeros though, we want it to be data dependent\n",
    "- we want to gather info from the past in data dependent way --> self-attention\n",
    "- **every single token will emit two vectors, a query vector and a key vector, aka what you are looking for and what do you contain**\n",
    "- affinities will be obtained with a dot product of query and key; that dot product becomes wei\n",
    "\n",
    "# Attention\n",
    "- Attention is a communication mechanism between nodes. It can be seen as nodes in a directed graph. Every node has a vector of information and it gets to aggregate information via a weighted sum from all the nodes that point to it. This is done in a data dependent manner. Attention can be applied to any arbitrary directed graph.\n",
    "- Our graph has 8 nodes since block size is 8. The first points to itself. The second node points to itself and the first node. etc.\n",
    "- Notice there is no notion of space. That's why we encode nodes to a specific position. This is different from convolution which has a specific layout in space. Attention is just a set of vector out there in space, and you have to add positional encodings to know the space.\n",
    "- In batch matrix multiplication, each example across batch dimensions is processed completely independently and never \"talk\" to each other. like separate pools. like 4 (batch size) separate pools of 8 (block size) nodes.\n",
    "- In an \"encoder\" attention block, just delete the single line that does the masking with trill, allowing all tokens to communicate. For different use case, maybe all tokens/nodes will all talk to each other. Maybe sentiment analysis. \n",
    "- In a \"decoder\" attention block, keep the triangular masking. This is usually used in autoregressive settings, like modeling.\n",
    "- \"self-attention\" means that the keys and values are produced from the same source x as queries. Node are looking at each other. In \"cross-attention\", key and value come from a separate external source, aka a separate source of nodes that we want to pool informaiton from.\n",
    "- \"Scaled\" dot-product attention includes additional division of wei by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "wei=tensor([[[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
      "         [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
      "         [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
      "\n",
      "        [[0.0443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0042, 0.0375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0560, 0.0210, 0.2496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3679, 0.1441, 0.4929, 0.0438, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0088, 0.1052, 0.0604, 0.5847, 0.2046, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0367, 0.0895, 0.0362, 0.2074, 0.1029, 0.0115, 0.0000, 0.0000],\n",
      "         [0.0480, 0.5010, 0.0172, 0.1434, 0.2807, 0.7090, 0.7318, 0.0000],\n",
      "         [0.4341, 0.1018, 0.1437, 0.0206, 0.4118, 0.2794, 0.2682, 1.0000]],\n",
      "\n",
      "        [[0.0419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0679, 0.0901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0119, 0.0392, 0.1158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0041, 0.5063, 0.1163, 0.1399, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7491, 0.0460, 0.2084, 0.0659, 0.0292, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0583, 0.1241, 0.2200, 0.0712, 0.2419, 0.1883, 0.0000, 0.0000],\n",
      "         [0.0107, 0.1200, 0.2721, 0.6404, 0.5979, 0.7420, 0.9713, 0.0000],\n",
      "         [0.0562, 0.0744, 0.0674, 0.0826, 0.1310, 0.0697, 0.0287, 1.0000]],\n",
      "\n",
      "        [[0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0937, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0881, 0.0591, 0.0066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0682, 0.0118, 0.0908, 0.0115, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0934, 0.0551, 0.0891, 0.1162, 0.0787, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3185, 0.6763, 0.0329, 0.3541, 0.3450, 0.1410, 0.0000, 0.0000],\n",
      "         [0.0340, 0.0079, 0.3160, 0.0306, 0.0840, 0.6004, 0.1996, 0.0000],\n",
      "         [0.0846, 0.1772, 0.4646, 0.4876, 0.4922, 0.2586, 0.8004, 1.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# VERSION 4: self-attention! for a single individual head\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention \n",
    "head_size = 16  # this is a hyperparameter\n",
    "key = nn.Linear(C, head_size, bias=False)  # this will apply matrix multiply with some fixed weights, bias is False\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# forward the model on x, no communication occurs here\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "# now all the queries will dot product with all the keys, aka communication\n",
    "# for every row of B, we'll have T^2 affinities\n",
    "# transpose the last 2 dimensions of k\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)  # exponentiate and normalize\n",
    "v = value(x)  # v is the vector we aggregate, instead of the raw x\n",
    "# think of x as private information to the head\n",
    "# for the purpose of a single head, v is the think that gets aggregated for this single head between the different nodes\n",
    "out = wei @ v\n",
    "\n",
    "print(f'{out.shape}')\n",
    "print(f'{wei=}')  # these are no longer uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naiive wei computation:\n",
      "k.var()=tensor(1.0449)\n",
      "q.var()=tensor(1.0700)\n",
      "wei.var()=tensor(17.4690)\n",
      "notice that variance of wei is 16 or 17\n",
      "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)=tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "\n",
      "now let' do scaled attention!!!:\n",
      "k.var()=tensor(1.0449)\n",
      "q.var()=tensor(1.0700)\n",
      "wei.var()=tensor(1.0918)\n",
      "notice that the variance of wei is about 1\n",
      "this is important because wei feeds into softmax, so we need wei to be fairly diffuse\n",
      "if wei takes on very negative or positive numbers, then softmax converges to very one hot vectors \n",
      "softmax sharpens towards the max, and softmax will be way too peaky, and then you aggregate from a single node\n",
      "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)=tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)=tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# SCALED DOT PRODUCT ATTENTION\n",
    "\n",
    "# if k and q are unit gaussian, and do wei naiively, just q @ k.transpose(-2,-1), then wei will be on the order of headsize.\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(\"naiive wei computation:\")\n",
    "print(f'{k.var()=}')\n",
    "print(f'{q.var()=}')\n",
    "print(f'{wei.var()=}') \n",
    "print(\"notice that variance of wei is 16 or 17\")\n",
    "print(f'{torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)=}')\n",
    "\n",
    "# now scale the attention by 1/sqrt of head size\n",
    "# then the vaiance of wei will be 1, it will be preserved\n",
    "print(\"\\nnow let' do scaled attention!!!:\")\n",
    "wei = q @ k.transpose(-2, -1)  * head_size**-0.5\n",
    "print(f'{k.var()=}')\n",
    "print(f'{q.var()=}')\n",
    "print(f'{wei.var()=}') \n",
    "print(\"notice that the variance of wei is about 1\")\n",
    "print(\"this is important because wei feeds into softmax, so we need wei to be fairly diffuse\")\n",
    "print(\"if wei takes on very negative or positive numbers, then softmax converges to very one hot vectors \")\n",
    "print(\"softmax sharpens towards the max, and softmax will be way too peaky, and then you aggregate from a single node\")\n",
    "print(f'{torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)=}')\n",
    "print(f'{torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "- apply multiple attentions in parallel and concatenating the results\n",
    "\n",
    "# Feed forward layer\n",
    "- follow self attention heads by a feed forward layer, aka let nodes talk to each other than think\n",
    "- feed forward layer is designed as a linear layer followed by a non-linearity, aka a simple multi-layer perceptron\n",
    "\n",
    "# Blocks\n",
    "- block contains communication (self attention heads) and computation (feed forward layer)\n",
    "\n",
    "# Deep neural networks\n",
    "- deep neural networks require some optimizations\n",
    "- optimization 1: skip connections\n",
    "- optimization 2: layer norm\n",
    "- optimization 3: dropout\n",
    "\n",
    "# Optimization 1: Addition of Residual Connection\n",
    "- Skip connections, also called residual connections\n",
    "- see paper \"Deep Residual Learning for Image Recognition\" which introduced the concept\n",
    "- you transform the data and then have a skip connection with addition to previous features\n",
    "- in other words, you have a residual pathway, you fork off from it, perfrom some computation, and then come back to the residual pathway and add to it, so you go from inputs to targets via plus and plus and plus\n",
    "- this is useful because during back propagation, addition distributes gradients equally to both of its branches that fed as the input\n",
    "- so gradients from loss hop throgh every addition node all the way to the input, and also fork off into the residual blocks\n",
    "- its a gradient super highway, unimpeded\n",
    "- residual pathways contirbute very little intially, and start to contribute more with time into training\n",
    "\n",
    "# Optimization 2: LayerNorm\n",
    "- this is implemennted in PyTorch\n",
    "- see paper \"Layer Normalization\"\n",
    "- this is similar to BatchNorm, which made sure that across batch dimensions, any indivdual neron had unit gaussian distribution, 0 mean and 1 standard deviation output\n",
    "- LayerNorm normalizes columns (while BatchNorm normalizes rows)\n",
    "- there is preNorm and postNorm formualations \n",
    "\n",
    "# Optimization 3: Dropout\n",
    "- dropput is something you can add right before the residual connection back into the residual pathway\n",
    "- this randomly prevents some of the nodes from communicating\n",
    "- see 2014 paper: \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"\n",
    "- it takes a neural network, eveyr forward backward path, shuts off some neurons and trains without then, because this change every time, it trains on an ensemble of sub networks\n",
    "- think of it as a regularization technique\n",
    "\n",
    "# Overfitting\n",
    "- when train loss start getting ahead of validation loss, aka train loss smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([32, 100])\n",
      "\n",
      "mean, std of one feature across all batch inputs\n",
      "look at each column\n",
      "x[:,0].mean()=tensor(0.1469), x[:,0].std()=tensor(0.8803)\n",
      "mean, std of a single input from the batch of its features\n",
      "look at each row\n",
      "x[0,:].mean()=tensor(2.3842e-09), x[0,:].std()=tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)  # like batch norm gain, set to ones by default\n",
    "        self.beta = torch.zeros(dim)  # like batch norm bias, set to zeros by default\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True, unbiased=True)  # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm(100)\n",
    "x = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "\n",
    "print(f'{x.shape=}\\n')\n",
    "print(\"mean, std of one feature across all batch inputs\")\n",
    "print(\"look at each column\")\n",
    "print(f'{x[:,0].mean()=}, {x[:,0].std()=}')\n",
    "print(\"mean, std of a single input from the batch of its features\")\n",
    "print(\"look at each row\")\n",
    "print(f'{x[0,:].mean()=}, {x[0,:].std()=}')\n",
    "\n",
    "# the difference between BatcHNorm and LayerNorm\n",
    "# Batch Norm normalizes on the rows\n",
    "# xmean = x.mean(0, keepdim=True),  xvar = x.var(0, keepdim=True, unbiased=True)\n",
    "# LayerNorm normalizes on the columns\n",
    "# xmean = x.mean(1, keepdim=True),  xvar = x.var(1, keepdim=True, unbiased=True)\n",
    "# we also don't need to update buffers.\n",
    "# there is no distinction between training and test time\n",
    "# we don't need momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "## Bigram model\n",
    "output from running *bigram.py* on my laptop\n",
    "```\n",
    "step 0: train loss 4.73047, vall loss 4.7241\n",
    "step 500: train loss 4.18133, vall loss 4.1848\n",
    "step 1000: train loss 3.73513, vall loss 3.7420\n",
    "step 1500: train loss 3.38599, vall loss 3.3942\n",
    "step 2000: train loss 3.12636, vall loss 3.1296\n",
    "step 2500: train loss 2.94339, vall loss 2.9434\n",
    "step 3000: train loss 2.80013, vall loss 2.8070\n",
    "step 3500: train loss 2.71062, vall loss 2.7112\n",
    "step 4000: train loss 2.64477, vall loss 2.6354\n",
    "step 4500: train loss 2.59551, vall loss 2.5975\n",
    "\n",
    "GofiO:\n",
    "Xro sick's q-etcichors lNSKIUWLLJ$ deposicea!\n",
    "SGINCAbor mealintimatede ser movis non. h;g oR a s, wind ngulffove ou!\n",
    "SSe na iravak:\n",
    "\n",
    "Buths d wans thasth bellout eshin,Ink'de ander yGo,\n",
    "JUEOWenqpor t th. lo CK$Ff eawhinr tonfur prerdy higse lom;Ay fr ILESiek'XEGHory bagure med anon:\n",
    "in oThis y'ld ben'd pond,N3Rocos$zFlim mereril,\n",
    "CENToPOLve!\n",
    "Fit CO:\n",
    "\n",
    "YMI&qupow ced -\n",
    "jouse, toe ic parl gum hangr t w fune hen seringououpeat? UveCOUDEP&y!qmmed t myeaverentcoV&ule whivf d.\n",
    "D n;\n",
    "'?sp\n",
    "Thad!zPHe!\n",
    "```\n",
    "hyperparameters used:\n",
    "```\n",
    "batch_size = 32  # number of independent sequences to process in parallel\n",
    "block_size = 8  #  the maximum context length for predictions\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 \n",
    "```\n",
    "## Decoder Transforer model\n",
    "output from **transformer.py** on my laptop\n",
    "```\n",
    "step 0: train loss 4.29086, vall loss 4.2774\n",
    "step 500: train loss 2.43082, vall loss 2.4306\n",
    "step 1000: train loss 2.30982, vall loss 2.3111\n",
    "step 1500: train loss 2.21267, vall loss 2.2409\n",
    "step 2000: train loss 2.18911, vall loss 2.1927\n",
    "step 2500: train loss 2.14461, vall loss 2.1689\n",
    "step 3000: train loss 2.11362, vall loss 2.1596\n",
    "step 3500: train loss 2.08868, vall loss 2.1466\n",
    "step 4000: train loss 2.06669, vall loss 2.1134\n",
    "step 4500: train loss 2.05117, vall loss 2.0971\n",
    "\n",
    "\n",
    "On thell ow; if hear of you whouse wiche hereight this qud, proot b ind you he ble to tim, and fard seesd to stat Yother make to furth minlies ospited:\n",
    "Frourn ip are, now Pod ffordor fores?\n",
    "\n",
    "ORUTUCE I woy so, for nish:\n",
    "To do word, so mift swet hum, sawell you promiorso thel spor!\n",
    "You HENR:\n",
    "The is souls seabd encais hof here.\n",
    "KING RA:\n",
    "Whal\n",
    "I we the ane,ught coning that wudan Howe eive\n",
    "Ale'ny is keard of lome a swilld must lust pearne't, and fome, whould I having to me\n",
    "'go go tiey thain ligese:\n",
    "U\n",
    "```\n",
    "hyperparameters used:\n",
    "```\n",
    "batch_size = 32  # number of independent sequences to process in parallel\n",
    "block_size = 8  #  the maximum context length for predictions\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3  # self-attention doesn't tolerate high learning rates\n",
    "n_embd = 32\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "```\n",
    "\n",
    "## Bigger Decoder Transformer Model - training on GPU\n",
    "EC2 instance was spun up (see README) with the **gd4n.xlarge** instance type. \n",
    "\n",
    "### Trail 1 with profiler, 1000 iterations\n",
    "hyperparameters used:\n",
    "```\n",
    "batch_size = 48  # number of independent sequences to process in parallel\n",
    "block_size = 50  #  the maximum context length for predictions\n",
    "max_iters = 1000\n",
    "eval_interval = 200\n",
    "learning_rate = 3e-4  # self-attention doesn't tolerate high learning rates\n",
    "```\n",
    "output:\n",
    "```\n",
    "step 0: train loss 4.23446, vall loss 4.2355\n",
    "step 200: train loss 3.01987, vall loss 3.0424\n",
    "step 400: train loss 2.70946, vall loss 2.7206\n",
    "step 600: train loss 2.59565, vall loss 2.5969\n",
    "step 800: train loss 2.52830, vall loss 2.5304\n",
    "\n",
    "WRLave, thethes iudt er bdehere oto Ralstobe be ara hot aiser tureseme ths wito bte t wis balery me oparireareu\n",
    "F e. he hady!\n",
    "Grt malither a, theseyher be rouchor te hariy Rm m pthase fe mrarhe,,\n",
    "Te withad beut wionte:\n",
    "Mar wengh t cho boveathan l inchathe w thiee; aig d,dildit Ge? omovedrshe be y mud nin\n",
    "Blelr. batshe t tothorede.\n",
    "TI bere met nouvONAHhe IOTENedeathan,\n",
    "Vid, t$ fos o it hisaffob \n",
    "\n",
    "E!P thareroal h whamitit th f ace thathevis m thept canethither obsde ale sto lere d tho pad,\n",
    "-ererin\n",
    "```\n",
    "output from profiler:\n",
    "```\n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                          MODEL_FORWARD         0.00%       0.000us         0.00%       0.000us       0.000us     780.734ms       210.70%     780.734ms      26.024ms            30  \n",
    "                                               aten::mm         5.96%     215.166ms         8.50%     306.660ms      28.238us     197.642ms        53.34%     197.648ms      18.200us         10860  \n",
    "       autograd::engine::evaluate_function: MmBackward0         0.71%      25.686ms         6.79%     244.958ms      75.604us       0.000us         0.00%     166.774ms      51.474us          3240  \n",
    "                                            MmBackward0         0.76%      27.467ms         6.08%     219.272ms      67.677us       0.000us         0.00%     166.774ms      51.474us          3240  \n",
    "                         volta_sgemm_32x32_sliced1x4_nt         0.00%       0.000us         0.00%       0.000us       0.000us     156.101ms        42.13%     156.101ms      45.644us          3420  \n",
    "                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us      99.523ms        26.86%      99.523ms       3.317ms            30  \n",
    "                                          ProfilerStep*         1.17%      42.315ms        69.42%        2.505s      80.804ms       0.000us         0.00%      84.931ms       2.740ms            31  \n",
    "                                          MODEL_FORWARD         5.15%     185.763ms        22.59%     814.999ms      27.167ms       0.000us         0.00%      82.771ms       2.759ms            30  \n",
    "                                              aten::bmm         3.54%     127.817ms         4.82%     173.906ms      26.837us      74.220ms        20.03%      74.234ms      11.456us          6480  \n",
    "      autograd::engine::evaluate_function: BmmBackward0         0.84%      30.300ms         4.68%     168.747ms      78.124us       0.000us         0.00%      52.554ms      24.330us          2160  \n",
    "                                           BmmBackward0         0.42%      14.987ms         3.84%     138.447ms      64.096us       0.000us         0.00%      52.554ms      24.330us          2160  \n",
    "                                           aten::matmul         0.86%      31.076ms         7.81%     281.917ms      52.207us       0.000us         0.00%      40.990ms       7.591us          5400  \n",
    "                                   volta_sgemm_64x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us      30.500ms         8.23%      30.500ms      14.120us          2160  \n",
    "                                   volta_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us      30.238ms         8.16%      30.238ms      13.999us          2160  \n",
    "                                           aten::linear         0.28%       9.944ms         6.16%     222.242ms      58.331us       0.000us         0.00%      28.742ms       7.544us          3810  \n",
    "    autograd::engine::evaluate_function: AddmmBackward0         0.27%       9.575ms         2.08%      75.114ms     131.779us       0.000us         0.00%      19.510ms      34.228us           570  \n",
    "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      19.305ms         5.21%      19.305ms       5.958us          3240  \n",
    "                                  volta_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      14.952ms         4.04%      14.952ms       4.119us          3630  \n",
    "                                   volta_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      13.482ms         3.64%      13.482ms       6.242us          2160  \n",
    "autograd::engine::evaluate_function: NativeLayerNorm...         0.22%       7.864ms         0.89%      32.136ms      82.401us       0.000us         0.00%      13.171ms      33.771us           390  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 3.608s\n",
    "Self CUDA time total: 370.551ms\n",
    "```\n",
    "### Trail 2 with profiler, bigger network, with profiler\n",
    "hyperparameters used:\n",
    "```\n",
    "batch_size = 48  # number of independent sequences to process in parallel\n",
    "block_size = 50  #  the maximum context length for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4  # self-attention doesn't tolerate high learning rates\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # ability to run on a gpu if present, and it'll be faster\n",
    "eval_iters = 200\n",
    "n_embd = 120\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2  # 20% disabled and drop to zero\n",
    "```\n",
    "output:\n",
    "```\n",
    "step 0: train loss 4.27598, vall loss 4.2730\n",
    "step 500: train loss 2.25223, vall loss 2.2637\n",
    "step 1000: train loss 2.00600, vall loss 2.0572\n",
    "step 1500: train loss 1.87084, vall loss 1.9646\n",
    "step 2000: train loss 1.77201, vall loss 1.8899\n",
    "step 2500: train loss 1.71431, vall loss 1.8538\n",
    "step 3000: train loss 1.65738, vall loss 1.8094\n",
    "step 3500: train loss 1.62083, vall loss 1.7834\n",
    "step 4000: train loss 1.59883, vall loss 1.7647\n",
    "step 4500: train loss 1.56548, vall loss 1.7389\n",
    "\n",
    "This in ganess are after! what me wran so olded:\n",
    "And sweet and fallows, What: you you turse,\n",
    "As my leess-world.\n",
    "\n",
    "RANT:\n",
    "\n",
    "Alacheld Retmed pace to whick chan sabull and out\n",
    "Thy day the bloody what gaques, and come, my hour lord?\n",
    "\n",
    "HENRY KING RIVENCENTIO:\n",
    "If that fee of hoppie insurion Compperane.\n",
    "\n",
    "SICINGS:\n",
    "wither, I dow, Herern'd he boing insale undeso hearts\n",
    "That had but fright the gramen hang? Which sive\n",
    "Thee make Mercous upon that I am thing\n",
    "this hild Romp.\n",
    "\n",
    "SANDINES:\n",
    "Then look thus, passiful, 't\n",
    "```\n",
    "output from profiler:\n",
    "```\n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                          MODEL_FORWARD         0.00%       0.000us         0.00%       0.000us       0.000us     259.484ms       189.43%     259.484ms      25.948ms            10  \n",
    "                                               aten::mm         6.02%      73.228ms         8.87%     107.852ms      29.793us      49.878ms        36.41%      49.898ms      13.784us          3620  \n",
    "                                          ProfilerStep*         1.14%      13.827ms        69.38%     843.911ms      76.719ms       0.000us         0.00%      48.788ms       4.435ms            11  \n",
    "                                          MODEL_FORWARD         5.14%      62.552ms        22.27%     270.815ms      27.082ms       0.000us         0.00%      43.885ms       4.388ms            10  \n",
    "                               Optimizer.step#Adam.step         0.00%       0.000us         0.00%       0.000us       0.000us      35.001ms        25.55%      35.001ms       3.500ms            10  \n",
    "                                              aten::bmm         3.51%      42.728ms         4.75%      57.775ms      26.748us      27.452ms        20.04%      27.452ms      12.709us          2160  \n",
    "       autograd::engine::evaluate_function: MmBackward0         0.70%       8.563ms         7.31%      88.938ms      82.350us       0.000us         0.00%      23.337ms      21.608us          1080  \n",
    "                                            MmBackward0         0.76%       9.268ms         6.61%      80.374ms      74.421us       0.000us         0.00%      23.337ms      21.608us          1080  \n",
    "                                           aten::linear         0.26%       3.159ms         5.98%      72.744ms      57.279us       0.000us         0.00%      20.630ms      16.244us          1270  \n",
    "                                           aten::matmul         0.84%      10.227ms         7.67%      93.253ms      51.807us       0.000us         0.00%      19.345ms      10.747us          1800  \n",
    "      autograd::engine::evaluate_function: BmmBackward0         0.79%       9.655ms         4.61%      56.089ms      77.902us       0.000us         0.00%      19.186ms      26.647us           720  \n",
    "                                           BmmBackward0         0.41%       5.032ms         3.82%      46.434ms      64.492us       0.000us         0.00%      19.186ms      26.647us           720  \n",
    "    autograd::engine::evaluate_function: AddmmBackward0         0.25%       3.028ms         2.07%      25.215ms     132.710us       0.000us         0.00%      19.097ms     100.509us           190  \n",
    "                                  volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us      16.167ms        11.80%      16.167ms      12.730us          1270  \n",
    "                                         AddmmBackward0         0.15%       1.863ms         1.26%      15.286ms      80.455us       0.000us         0.00%      15.483ms      81.487us           190  \n",
    "                                   volta_sgemm_64x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us      13.419ms         9.80%      13.419ms      17.204us           780  \n",
    "                         volta_sgemm_64x32_sliced1x4_nt         0.00%       0.000us         0.00%       0.000us       0.000us      12.038ms         8.79%      12.038ms      11.146us          1080  \n",
    "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      11.059ms         8.07%      11.059ms      10.240us          1080  \n",
    "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.571ms         7.72%      10.571ms       8.390us          1260  \n",
    "                                   volta_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us      10.262ms         7.49%      10.262ms      14.252us           720  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 1.216s\n",
    "Self CUDA time total: 136.978ms\n",
    "```\n",
    "\n",
    "## Summary\n",
    "- The bigram model on cpu achieved a validation loss of about 2.6\n",
    "- The decoder transformer model on cpu achieved a significantly better loss of about 2.1\n",
    "- The decoder transformer model's hyperparameters was updated to create an even bigger network. This resulted in a loss of about 1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
